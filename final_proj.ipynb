{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark as ps\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    print(\"initializing spark...\")\n",
    "    try:\n",
    "        sc = ps.SparkContext('local[*]')\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print(\"Just created a SparkContext\")\n",
    "    except ValueError:\n",
    "        warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fileName):\n",
    "    print(\"reading csv file...\")\n",
    "    df=spark.read.csv(fileName,sep=\",\",inferSchema=True,header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    print(\"preprocessing...\")\n",
    "#     df.printSchema()\n",
    "#     df.show(truncate=False)\n",
    "    df.count()\n",
    "    \n",
    "    df1=df.withColumnRenamed('_c0',\"id\").withColumnRenamed('_c1','label').withColumnRenamed('_c2','tweet')\n",
    "#     df1.printSchema()\n",
    "#     df1.show(truncate=False)\n",
    "    \n",
    "    df2 = df1.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "#     df2.show(truncate=False)\n",
    "    \n",
    "    df3 = df2.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "#     df3.show(truncate=False)\n",
    "    \n",
    "    df4 = df3.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "#     df4.show(truncate=False)\n",
    "    \n",
    "    df5 = df4.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "#     df5.show(truncate=False)\n",
    "    \n",
    "    df6 = df5.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "#     df6.show(truncate=False)\n",
    "    \n",
    "    tokenizer=Tokenizer(inputCol=\"tweet\",outputCol=\"words\")\n",
    "    wordData=tokenizer.transform(df6)\n",
    "#     wordData.show()\n",
    "    \n",
    "    remover=StopWordsRemover(inputCol=\"words\",outputCol=\"word_clean\")\n",
    "    word_clean_data=remover.transform(wordData)\n",
    "#     word_clean_data.show()\n",
    "    \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    count=CountVectorizer(inputCol=\"word_clean\",outputCol=\"rawFeatures\")\n",
    "#     print(count)\n",
    "    \n",
    "    model=count.fit(word_clean_data)\n",
    "#     print(model)\n",
    "    \n",
    "    featurizedData=model.transform(word_clean_data)\n",
    "#     featurizedData.show()\n",
    "    idf=IDF(inputCol=\"rawFeatures\",outputCol=\"features\")\n",
    "    idfModel=idf.fit(featurizedData)\n",
    "    rescaledData=idfModel.transform(featurizedData)\n",
    "#     rescaledData.select(\"label\",\"features\").show()\n",
    "    return rescaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    print(\"splitting dataset...\")\n",
    "    seed=0\n",
    "    trainDf,testDf=df.randomSplit([0.7,0.3],seed)\n",
    "    trainDf.count()\n",
    "    testDf.count()\n",
    "    return trainDf,testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_data,test_data):\n",
    "    print(\"Using logistic regression model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    \n",
    "    lr = LogisticRegression(maxIter=15)\n",
    "    paramGrid_lr = ParamGridBuilder().build()\n",
    "    \n",
    "    crossval_lr = CrossValidator(estimator=lr,estimatorParamMaps=paramGrid_lr,evaluator=BinaryClassificationEvaluator(),numFolds=8)\n",
    "    cvModel_lr = crossval_lr.fit(train_data)\n",
    "    best_model_lr = cvModel_lr.bestModel.summary\n",
    "    \n",
    "    report = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"label\",metricName=\"areaUnderROC\")\n",
    "    p1 = report.evaluate(best_model_lr.predictions)\n",
    "    \n",
    "    pred_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"label\",metricName=\"f1\")\n",
    "    p2 = pred_lr.evaluate(best_model_lr.predictions)\n",
    "    \n",
    "    pred_lr = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"label\",metricName=\"accuracy\")\n",
    "    p3 = pred_lr.evaluate(best_model_lr.predictions)\n",
    "    \n",
    "    train_fit_lr = best_model_lr.predictions.select('label','prediction')\n",
    "    print(train_fit_lr.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    predictions_lr = cvModel_lr.transform(test_data)\n",
    "    print(predictions_lr.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_lr = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4=my_eval_lr.evaluate(predictions_lr)\n",
    "    \n",
    "    my_mc_lr = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p5=my_mc_lr.evaluate(predictions_lr)\n",
    "    \n",
    "    my_mc_lr = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p6=my_mc_lr.evaluate(predictions_lr)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_data,test_data):\n",
    "    print(\"Using SVM model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    \n",
    "    svm = LinearSVC(maxIter=5,regParam=0.01)\n",
    "    model = svm.fit(train_data)\n",
    "    \n",
    "    train_pred = model.transform(train_data)\n",
    "    print(train_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_svm = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p1 = my_eval_svm.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p2 = my_mc_svm.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p3 = my_mc_svm.evaluate(train_pred)\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    test_pred = model.transform(test_data)\n",
    "    print(test_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluation = evaluator.evaluate(test_pred)\n",
    "    \n",
    "    my_eval_svm = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4 = my_eval_svm.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p5 = my_mc_svm.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p6 = my_mc_svm.evaluate(test_pred)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    \n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_Bayes(train_data,test_data):\n",
    "    print(\"Using Naive Bayes model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    nb = NaiveBayes()\n",
    "    \n",
    "    paramGrid_nb = ParamGridBuilder() \\\n",
    "        .addGrid(nb.smoothing, np.linspace(0.3, 10, 10)) \\\n",
    "        .build()\n",
    "    crossval_nb = CrossValidator(estimator=nb, estimatorParamMaps=paramGrid_nb, evaluator=BinaryClassificationEvaluator(), numFolds= 5)\n",
    "    cvModel_nb = crossval_nb.fit(train_data)\n",
    "    \n",
    "    train_pred = cvModel_nb.transform(train_data)\n",
    "    print(train_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_nb = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p1 = my_eval_nb.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p2 = my_mc_nb.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p3 = my_mc_nb.evaluate(train_pred)\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    predictions_nb = cvModel_nb.transform(test_data)\n",
    "    print(predictions_nb.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_nb = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4 = my_eval_nb.evaluate(predictions_nb)\n",
    "    \n",
    "    my_mc_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p5 = my_mc_nb.evaluate(predictions_nb)\n",
    "    \n",
    "    my_mc_nb = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p6 = my_mc_nb.evaluate(predictions_nb)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    \n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Forest(train_data,test_data):\n",
    "    print(\"Using Random Forest model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_model = rf.fit(train_data)\n",
    "    \n",
    "    train_pred = rf_model.transform(train_data)\n",
    "    print(train_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_rf = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p1 = my_eval_rf.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p2 = my_mc_rf.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p3 = my_mc_rf.evaluate(train_pred)\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    test_pred = rf_model.transform(test_data)\n",
    "    print(test_pred.groupBy('label','prediction').count().show())\n",
    "    my_eval_rf = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4 = my_eval_rf.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p5 = my_mc_rf.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p6 = my_mc_rf.evaluate(test_pred)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark...\n",
      "reading csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: SparkContext already exists in this scope\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "init_spark()\n",
    "df=read_file(\"twitter.csv\")\n",
    "df=pre_process(df)\n",
    "train_data,test_data=train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SVM model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|20733|\n",
      "|    1|       0.0|  198|\n",
      "|    1|       1.0| 1375|\n",
      "|    0|       1.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  370|\n",
      "|    0|       0.0| 8948|\n",
      "|    1|       1.0|  306|\n",
      "|    0|       1.0|   28|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "{'ROC': 0.9369664910967898, 'F1': 0.9906693279729695, 'Accuracy': 0.9909457642312864}\n",
      "{'ROC': 0.7247716461517366, 'F1': 0.9521690921806005, 'Accuracy': 0.9587650227932035}\n",
      "Using Naive Bayes model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|20316|\n",
      "|    1|       1.0| 1546|\n",
      "|    1|       0.0|   27|\n",
      "|    0|       1.0|  421|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  109|\n",
      "|    0|       0.0| 7363|\n",
      "|    0|       1.0| 1613|\n",
      "|    1|       1.0|  567|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "{'ROC': 0.9812667352988343, 'F1': 0.9809405056058967, 'Accuracy': 0.9799193186911699}\n",
      "{'ROC': 0.8295279852123744, 'F1': 0.8604105048405976, 'Accuracy': 0.8215913800248653}\n",
      "Using logistic regression model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       0.0|20737|\n",
      "|  1.0|       1.0| 1567|\n",
      "|  1.0|       0.0|    6|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  283|\n",
      "|    0|       0.0| 8806|\n",
      "|    1|       1.0|  393|\n",
      "|    0|       1.0|  170|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "{'ROC': 0.9980928162746345, 'F1': 0.9997308248077091, 'Accuracy': 0.9997310623038996}\n",
      "{'ROC': 0.781210776403084, 'F1': 0.9530667219229175, 'Accuracy': 0.9510732921109564}\n",
      "Using Random Forest model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|20737|\n",
      "|    1|       0.0| 1573|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  676|\n",
      "|    0|       0.0| 8976|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "{'ROC': 0.5, 'F1': 0.8955284560337493, 'Accuracy': 0.9294935006723443}\n",
      "{'ROC': 0.5, 'F1': 0.8962148607931361, 'Accuracy': 0.9299627020306672}\n"
     ]
    }
   ],
   "source": [
    "train_summary,test_summary=SVM(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)\n",
    "\n",
    "train_summary,test_summary=naive_Bayes(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)\n",
    "\n",
    "train_summary,test_summary=logistic_regression(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)\n",
    "\n",
    "train_summary,test_summary=random_Forest(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
