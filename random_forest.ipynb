{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark as ps\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def init_spark():\n",
    "    print(\"initializing spark...\")\n",
    "    try:\n",
    "        sc = ps.SparkContext('local[*]')\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print(\"Just created a SparkContext\")\n",
    "    except ValueError:\n",
    "        warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_file(fileName):\n",
    "    print(\"reading csv file...\")\n",
    "    df=spark.read.csv(fileName,sep=\",\",inferSchema=True,header=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    print(\"preprocessing...\")\n",
    "    df.count()\n",
    "    df1=df.withColumnRenamed('_c0',\"id\").withColumnRenamed('_c1','label').withColumnRenamed('_c2','tweet')\n",
    "    df2 = df1.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "    df3 = df2.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "    df4 = df3.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "    df5 = df4.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "    df6 = df5.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "    tokenizer=Tokenizer(inputCol=\"tweet\",outputCol=\"words\")\n",
    "    wordData=tokenizer.transform(df6)\n",
    "    remover=StopWordsRemover(inputCol=\"words\",outputCol=\"word_clean\")\n",
    "    word_clean_data=remover.transform(wordData)\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    count=CountVectorizer(inputCol=\"word_clean\",outputCol=\"rawFeatures\")\n",
    "    model=count.fit(word_clean_data)\n",
    "    featurizedData=model.transform(word_clean_data)\n",
    "    idf=IDF(inputCol=\"rawFeatures\",outputCol=\"features\")\n",
    "    idfModel=idf.fit(featurizedData)\n",
    "    rescaledData=idfModel.transform(featurizedData)\n",
    "    return rescaledData\n",
    "\n",
    "\n",
    "def train_test_split(df,train=0.7,test=0.3):\n",
    "    print(\"splitting dataset...\")\n",
    "    seed=0\n",
    "    trainDf,testDf=df.randomSplit([train,test],seed)\n",
    "    trainDf.count()\n",
    "    testDf.count()\n",
    "    return trainDf,testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Forest(train_data,test_data):\n",
    "    print(\"................................................................................................\")\n",
    "    print(\"Using Random Forest model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_model = rf.fit(train_data)\n",
    "    \n",
    "    train_pred = rf_model.transform(train_data)\n",
    "    print(train_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_rf = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p1 = my_eval_rf.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p2 = my_mc_rf.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p3 = my_mc_rf.evaluate(train_pred)\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    test_pred = rf_model.transform(test_data)\n",
    "    print(test_pred.groupBy('label','prediction').count().show())\n",
    "    my_eval_rf = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4 = my_eval_rf.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p5 = my_mc_rf.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_rf = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p6 = my_mc_rf.evaluate(test_pred)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    print(\"................................................................................................\")\n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark...\n",
      "reading csv file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: SparkContext already exists in this scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "init_spark()\n",
    "df=read_file(\"twitter.csv\")\n",
    "df=pre_process(df)\n",
    "train_data,test_data=train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................\n",
      "................................................................................................\n",
      "Using Random Forest model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|20737|\n",
      "|    1|       0.0| 1573|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  676|\n",
      "|    0|       0.0| 8976|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "................................................................................................\n",
      "{'ROC': 0.5, 'F1': 0.8955284560337493, 'Accuracy': 0.9294935006723443}\n",
      "{'ROC': 0.5, 'F1': 0.8962148607931361, 'Accuracy': 0.9299627020306672}\n",
      "Time elapsed: 10.163563191890717  mins.\n",
      ".........................................................................................\n"
     ]
    }
   ],
   "source": [
    "print(\".........................................................................................\")\n",
    "import time\n",
    "start=time.time()\n",
    "train_summary,test_summary=random_Forest(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)\n",
    "elapsed=time.time()-start\n",
    "print(\"Time elapsed:\",elapsed/60,\" mins.\")\n",
    "print(\".........................................................................................\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
