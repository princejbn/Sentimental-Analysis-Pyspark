{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark as ps\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def init_spark():\n",
    "    print(\"initializing spark...\")\n",
    "    try:\n",
    "        sc = ps.SparkContext('local[*]')\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print(\"Just created a SparkContext\")\n",
    "    except ValueError:\n",
    "        warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "def read_file(fileName):\n",
    "    print(\"reading csv file...\")\n",
    "    df=spark.read.csv(fileName,sep=\",\",inferSchema=True,header=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def pre_process(df):\n",
    "    print(\"preprocessing...\")\n",
    "    df.count()\n",
    "    df1=df.withColumnRenamed('_c0',\"id\").withColumnRenamed('_c1','label').withColumnRenamed('_c2','tweet')\n",
    "    df2 = df1.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "    df3 = df2.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "    df4 = df3.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "    df5 = df4.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "    df6 = df5.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "    tokenizer=Tokenizer(inputCol=\"tweet\",outputCol=\"words\")\n",
    "    wordData=tokenizer.transform(df6)\n",
    "    remover=StopWordsRemover(inputCol=\"words\",outputCol=\"word_clean\")\n",
    "    word_clean_data=remover.transform(wordData)\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    count=CountVectorizer(inputCol=\"word_clean\",outputCol=\"rawFeatures\")\n",
    "    model=count.fit(word_clean_data)\n",
    "    featurizedData=model.transform(word_clean_data)\n",
    "    idf=IDF(inputCol=\"rawFeatures\",outputCol=\"features\")\n",
    "    idfModel=idf.fit(featurizedData)\n",
    "    rescaledData=idfModel.transform(featurizedData)\n",
    "    return rescaledData\n",
    "\n",
    "\n",
    "def train_test_split(df,train=0.7,test=0.3):\n",
    "    print(\"splitting dataset...\")\n",
    "    seed=0\n",
    "    trainDf,testDf=df.randomSplit([train,test],seed)\n",
    "    trainDf.count()\n",
    "    testDf.count()\n",
    "    return trainDf,testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(train_data,test_data):\n",
    "    print(\"................................................................................................\")\n",
    "    print(\"Using SVM model with test_data...\")\n",
    "    d1 = {}\n",
    "    d2 = {}\n",
    "    \n",
    "    svm = LinearSVC(maxIter=5,regParam=0.01)\n",
    "    model = svm.fit(train_data)\n",
    "    \n",
    "    train_pred = model.transform(train_data)\n",
    "    print(train_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    my_eval_svm = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p1 = my_eval_svm.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p2 = my_mc_svm.evaluate(train_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p3 = my_mc_svm.evaluate(train_pred)\n",
    "    \n",
    "    d1['ROC'] = p1\n",
    "    d1['F1'] = p2\n",
    "    d1['Accuracy'] = p3\n",
    "    \n",
    "    test_pred = model.transform(test_data)\n",
    "    print(test_pred.groupBy('label','prediction').count().show())\n",
    "    \n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    evaluation = evaluator.evaluate(test_pred)\n",
    "    \n",
    "    my_eval_svm = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label', metricName='areaUnderROC')\n",
    "    p4 = my_eval_svm.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')\n",
    "    p5 = my_mc_svm.evaluate(test_pred)\n",
    "    \n",
    "    my_mc_svm = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "    p6 = my_mc_svm.evaluate(test_pred)\n",
    "    \n",
    "    d2['ROC'] = p4\n",
    "    d2['F1']= p5\n",
    "    d2['Accuracy'] = p6\n",
    "    print(\"................................................................................................\")\n",
    "    return d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing spark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: SparkContext already exists in this scope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv file...\n",
      "preprocessing...\n",
      "splitting dataset...\n"
     ]
    }
   ],
   "source": [
    "init_spark()\n",
    "df=read_file(\"twitter.csv\")\n",
    "df=pre_process(df)\n",
    "train_data,test_data=train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processed tweets\n",
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|label|               tweet|               words|          word_clean|         rawFeatures|            features|\n",
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|    1| user when a fath...|[, user, when, a,...|[, user, father, ...|(39572,[0,1,161,1...|(39572,[0,1,161,1...|\n",
      "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|(39572,[0,1,19,22...|(39572,[0,1,19,22...|\n",
      "|  3|    1|user that was fuc...|[user, that, was,...|[user, fucking, w...|(39572,[1,525,126...|(39572,[1,525,126...|\n",
      "|  4|    1|userthat was so s...|[userthat, was, s...|  [userthat, shitty]|(39572,[2934,3100...|(39572,[2934,3100...|\n",
      "|  5|    0| factsguide socie...|[, factsguide, so...|[, factsguide, so...|(39572,[0,195,149...|(39572,[0,195,149...|\n",
      "|  6|    0| huge fan fare an...|[, huge, fan, far...|[, huge, fan, far...|(39572,[0,15,107,...|(39572,[0,15,107,...|\n",
      "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|(39572,[0,1,57,19...|(39572,[0,1,57,19...|\n",
      "|  8|    0|the next school y...|[the, next, schoo...|[next, school, ye...|(39572,[0,19,63,7...|(39572,[0,19,63,7...|\n",
      "|  9|    0|we won love the l...|[we, won, love, t...|[won, love, land,...|(39572,[2,747,983...|(39572,[2,747,983...|\n",
      "| 10|    1|   you are so boring|[, you, are, so, ...|          [, boring]|(39572,[0,1979],[...|(39572,[0,1979],[...|\n",
      "| 11|    0|  ireland consume...|[, , ireland, con...|[, , ireland, con...|(39572,[0,66,88,9...|(39572,[0,66,88,9...|\n",
      "| 12|    0|we are so selfish...|[we, are, so, sel...|[selfish, orlando...|(39572,[0,2,61,68...|(39572,[0,2,61,68...|\n",
      "| 13|    0|i get to see my d...|[i, get, to, see,...|[get, see, daddy,...|(39572,[0,11,15,2...|(39572,[0,11,15,2...|\n",
      "| 14|    1|  i wont spare you  |[i, wont, spare, ...|       [wont, spare]|(39572,[268,5830]...|(39572,[268,5830]...|\n",
      "| 15|    1|it was not good f...|[it, was, not, go...|              [good]|  (39572,[18],[1.0])|(39572,[18],[3.66...|\n",
      "| 16|    1|ouchjunior is ang...|[ouchjunior, is, ...|[ouchjunior, angr...|(39572,[0,308,436...|(39572,[0,308,436...|\n",
      "| 17|    0|i am thankful for...|[i, am, thankful,...|[thankful, paner,...|(39572,[13,14,164...|(39572,[13,14,164...|\n",
      "| 18|    1|retweet if you ag...|[retweet, if, you...|    [retweet, agree]|(39572,[335,787],...|(39572,[335,787],...|\n",
      "| 19|    0|its friday  smile...|[its, friday, , s...|[friday, , smiles...|(39572,[0,1,16,33...|(39572,[0,1,16,33...|\n",
      "| 20|    0|as we all know es...|[as, we, all, kno...|[know, essential,...|(39572,[55,138,17...|(39572,[55,138,17...|\n",
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processed tweets\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................................................\n",
      "................................................................................................\n",
      "Using SVM model with test_data...\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0|20733|\n",
      "|    1|       0.0|  198|\n",
      "|    1|       1.0| 1375|\n",
      "|    0|       1.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|  370|\n",
      "|    0|       0.0| 8948|\n",
      "|    1|       1.0|  306|\n",
      "|    0|       1.0|   28|\n",
      "+-----+----------+-----+\n",
      "\n",
      "None\n",
      "................................................................................................\n",
      "{'ROC': 0.9369664910967898, 'F1': 0.9906693279729695, 'Accuracy': 0.9909457642312864}\n",
      "{'ROC': 0.7247716461517366, 'F1': 0.9521690921806005, 'Accuracy': 0.9587650227932035}\n",
      "Time elapsed: 3.844720975557963  mins.\n",
      ".........................................................................................\n"
     ]
    }
   ],
   "source": [
    "print(\".........................................................................................\")\n",
    "import time\n",
    "start=time.time()\n",
    "train_summary,test_summary=SVM(train_data,test_data)\n",
    "print(train_summary)\n",
    "print(test_summary)\n",
    "elapsed=time.time()-start\n",
    "print(\"Time elapsed:\",elapsed/60,\" mins.\")\n",
    "print(\".........................................................................................\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
